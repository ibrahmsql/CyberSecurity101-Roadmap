# ðŸ¦  Malware Analizi ve Tersine MÃ¼hendislik

## ðŸ“‹ ModÃ¼l Genel BakÄ±ÅŸ

### ðŸŽ¯ Ã–ÄŸrenme Hedefleri

**Teorik Bilgi**:
- Malware tÃ¼rleri ve sÄ±nÄ±flandÄ±rmasÄ±
- Statik ve dinamik analiz teknikleri
- Tersine mÃ¼hendislik metodolojileri
- Anti-analysis teknikleri ve bypass yÃ¶ntemleri
- Malware ailesi tanÄ±mlama

**Pratik Beceriler**:
- IDA Pro, Ghidra, x64dbg kullanÄ±mÄ±
- Assembly dili okuma ve anlama
- Packed malware unpacking
- Network traffic analizi
- Memory forensics

**Teknik Yetkinlikler**:
- PE/ELF dosya formatlarÄ±
- Windows/Linux sistem internals
- Cryptographic analysis
- Behavioral analysis
- Threat intelligence integration

### ðŸŒ GerÃ§ek DÃ¼nya UygulamalarÄ±

**Siber GÃ¼venlik Åžirketleri**:
- Incident response ekiplerinde malware analizi
- Threat hunting operasyonlarÄ±
- APT grup aktivite takibi
- Zero-day exploit analizi

**Kurumsal GÃ¼venlik**:
- Internal threat detection
- Forensic investigation
- Security awareness training
- Threat intelligence production

**AraÅŸtÄ±rma ve GeliÅŸtirme**:
- Yeni malware ailesi keÅŸfi
- Anti-malware Ã§Ã¶zÃ¼m geliÅŸtirme
- Vulnerability research
- Academic research

## ðŸ“š Teorik Temeller

### ðŸ” Malware SÄ±nÄ±flandÄ±rmasÄ±

#### Propagation Method (YayÄ±lma YÃ¶ntemi)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MALWARE TAXONOMY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   VIRUSES   â”‚  â”‚    WORMS    â”‚  â”‚   TROJANS   â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚
â”‚  â”‚ â€¢ File      â”‚  â”‚ â€¢ Network   â”‚  â”‚ â€¢ Backdoor  â”‚        â”‚
â”‚  â”‚ â€¢ Boot      â”‚  â”‚ â€¢ Email     â”‚  â”‚ â€¢ Spyware   â”‚        â”‚
â”‚  â”‚ â€¢ Macro     â”‚  â”‚ â€¢ USB       â”‚  â”‚ â€¢ Keylogger â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  RANSOMWARE â”‚  â”‚   ROOTKITS  â”‚  â”‚   ADWARE    â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚
â”‚  â”‚ â€¢ Crypto    â”‚  â”‚ â€¢ Kernel    â”‚  â”‚ â€¢ Browser   â”‚        â”‚
â”‚  â”‚ â€¢ Locker    â”‚  â”‚ â€¢ User      â”‚  â”‚ â€¢ System    â”‚        â”‚
â”‚  â”‚ â€¢ Hybrid    â”‚  â”‚ â€¢ Firmware  â”‚  â”‚ â€¢ Mobile    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Malware Lifecycle
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MALWARE LIFECYCLE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. DEVELOPMENT  â†’  2. DISTRIBUTION  â†’  3. INFECTION       â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ Coding  â”‚        â”‚ Hosting â”‚        â”‚ Exploit â”‚      â”‚
â”‚     â”‚ Testing â”‚        â”‚ Spam    â”‚        â”‚ Execute â”‚      â”‚
â”‚     â”‚ Packing â”‚        â”‚ Social  â”‚        â”‚ Install â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  4. PERSISTENCE  â†’  5. EXECUTION  â†’  6. COMMUNICATION     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ Registryâ”‚        â”‚ Payload â”‚        â”‚ C&C     â”‚      â”‚
â”‚     â”‚ Service â”‚        â”‚ Data    â”‚        â”‚ Exfil   â”‚      â”‚
â”‚     â”‚ Startup â”‚        â”‚ Spread  â”‚        â”‚ Update  â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”¬ Analiz Metodolojileri

#### Static Analysis Workflow
```python
# Static Analysis Framework
import pefile
import hashlib
import yara
import ssdeep
from capstone import *
import re

class StaticAnalyzer:
    def __init__(self, file_path):
        self.file_path = file_path
        self.pe = None
        self.analysis_results = {}
        
    def basic_file_info(self):
        """Extract basic file information"""
        with open(self.file_path, 'rb') as f:
            data = f.read()
            
        self.analysis_results['file_size'] = len(data)
        self.analysis_results['md5'] = hashlib.md5(data).hexdigest()
        self.analysis_results['sha1'] = hashlib.sha1(data).hexdigest()
        self.analysis_results['sha256'] = hashlib.sha256(data).hexdigest()
        self.analysis_results['ssdeep'] = ssdeep.hash(data)
        
        return self.analysis_results
    
    def pe_analysis(self):
        """Analyze PE file structure"""
        try:
            self.pe = pefile.PE(self.file_path)
            
            # Basic PE info
            pe_info = {
                'machine': hex(self.pe.FILE_HEADER.Machine),
                'timestamp': self.pe.FILE_HEADER.TimeDateStamp,
                'sections': len(self.pe.sections),
                'entry_point': hex(self.pe.OPTIONAL_HEADER.AddressOfEntryPoint),
                'image_base': hex(self.pe.OPTIONAL_HEADER.ImageBase)
            }
            
            # Sections analysis
            sections = []
            for section in self.pe.sections:
                section_info = {
                    'name': section.Name.decode().rstrip('\x00'),
                    'virtual_address': hex(section.VirtualAddress),
                    'virtual_size': section.Misc_VirtualSize,
                    'raw_size': section.SizeOfRawData,
                    'entropy': section.get_entropy()
                }
                sections.append(section_info)
            
            pe_info['sections_detail'] = sections
            
            # Imports analysis
            imports = []
            if hasattr(self.pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in self.pe.DIRECTORY_ENTRY_IMPORT:
                    dll_imports = {
                        'dll': entry.dll.decode(),
                        'functions': []
                    }
                    for imp in entry.imports:
                        if imp.name:
                            dll_imports['functions'].append(imp.name.decode())
                    imports.append(dll_imports)
            
            pe_info['imports'] = imports
            
            self.analysis_results['pe_analysis'] = pe_info
            
        except Exception as e:
            self.analysis_results['pe_analysis'] = f"Error: {str(e)}"
        
        return self.analysis_results
    
    def string_analysis(self):
        """Extract and analyze strings"""
        with open(self.file_path, 'rb') as f:
            data = f.read()
        
        # ASCII strings
        ascii_strings = re.findall(b'[\x20-\x7E]{4,}', data)
        unicode_strings = re.findall(b'(?:[\x20-\x7E]\x00){4,}', data)
        
        # Interesting patterns
        interesting_patterns = {
            'urls': re.findall(b'https?://[\x20-\x7E]+', data),
            'ips': re.findall(b'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', data),
            'emails': re.findall(b'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', data),
            'registry_keys': re.findall(b'HKEY_[A-Z_]+\\\\[\x20-\x7E]+', data)
        }
        
        self.analysis_results['strings'] = {
            'ascii_count': len(ascii_strings),
            'unicode_count': len(unicode_strings),
            'interesting_patterns': interesting_patterns
        }
        
        return self.analysis_results
    
    def yara_scan(self, rules_path):
        """Scan with YARA rules"""
        try:
            rules = yara.compile(filepath=rules_path)
            matches = rules.match(self.file_path)
            
            yara_results = []
            for match in matches:
                yara_results.append({
                    'rule': match.rule,
                    'tags': match.tags,
                    'meta': match.meta
                })
            
            self.analysis_results['yara_matches'] = yara_results
            
        except Exception as e:
            self.analysis_results['yara_matches'] = f"Error: {str(e)}"
        
        return self.analysis_results
    
    def disassemble_entry_point(self, instruction_count=50):
        """Disassemble code at entry point"""
        if not self.pe:
            return "PE not loaded"
        
        try:
            # Get entry point RVA
            entry_point = self.pe.OPTIONAL_HEADER.AddressOfEntryPoint
            
            # Get raw offset
            raw_offset = self.pe.get_offset_from_rva(entry_point)
            
            # Read code
            code = self.pe.get_data(entry_point, 200)  # Read 200 bytes
            
            # Disassemble
            md = Cs(CS_ARCH_X86, CS_MODE_32)
            instructions = []
            
            for i, instruction in enumerate(md.disasm(code, entry_point)):
                if i >= instruction_count:
                    break
                instructions.append({
                    'address': hex(instruction.address),
                    'mnemonic': instruction.mnemonic,
                    'op_str': instruction.op_str,
                    'bytes': instruction.bytes.hex()
                })
            
            self.analysis_results['disassembly'] = instructions
            
        except Exception as e:
            self.analysis_results['disassembly'] = f"Error: {str(e)}"
        
        return self.analysis_results
    
    def generate_report(self):
        """Generate comprehensive analysis report"""
        self.basic_file_info()
        self.pe_analysis()
        self.string_analysis()
        self.disassemble_entry_point()
        
        return self.analysis_results

# Usage example
analyzer = StaticAnalyzer("/path/to/malware.exe")
results = analyzer.generate_report()
```

#### Dynamic Analysis Framework
```python
# Dynamic Analysis with Process Monitoring
import psutil
import time
import json
import subprocess
from threading import Thread
import winreg
import os

class DynamicAnalyzer:
    def __init__(self, malware_path, analysis_duration=300):
        self.malware_path = malware_path
        self.analysis_duration = analysis_duration
        self.monitoring_data = {
            'process_activity': [],
            'file_operations': [],
            'network_connections': [],
            'registry_changes': [],
            'system_changes': []
        }
        self.baseline_processes = set()
        self.baseline_files = set()
        self.monitoring = False
        
    def take_baseline(self):
        """Take system baseline before execution"""
        # Baseline processes
        for proc in psutil.process_iter(['pid', 'name']):
            try:
                self.baseline_processes.add(proc.info['name'])
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        # Baseline files (sample directory)
        sample_dirs = ['C:\\Windows\\System32', 'C:\\Windows\\Temp']
        for directory in sample_dirs:
            if os.path.exists(directory):
                for root, dirs, files in os.walk(directory):
                    for file in files:
                        self.baseline_files.add(os.path.join(root, file))
    
    def monitor_processes(self):
        """Monitor process creation and termination"""
        while self.monitoring:
            current_processes = set()
            for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time']):
                try:
                    proc_info = proc.info
                    current_processes.add(proc_info['name'])
                    
                    # New process detected
                    if proc_info['name'] not in self.baseline_processes:
                        self.monitoring_data['process_activity'].append({
                            'timestamp': time.time(),
                            'action': 'created',
                            'pid': proc_info['pid'],
                            'name': proc_info['name'],
                            'cmdline': proc_info['cmdline'],
                            'create_time': proc_info['create_time']
                        })
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Check for terminated processes
            for proc_name in self.baseline_processes:
                if proc_name not in current_processes:
                    self.monitoring_data['process_activity'].append({
                        'timestamp': time.time(),
                        'action': 'terminated',
                        'name': proc_name
                    })
            
            self.baseline_processes = current_processes
            time.sleep(1)
    
    def monitor_network(self):
        """Monitor network connections"""
        baseline_connections = set()
        
        # Get baseline connections
        for conn in psutil.net_connections():
            if conn.status == 'ESTABLISHED':
                baseline_connections.add((conn.laddr, conn.raddr))
        
        while self.monitoring:
            current_connections = set()
            for conn in psutil.net_connections():
                try:
                    if conn.status == 'ESTABLISHED':
                        conn_tuple = (conn.laddr, conn.raddr)
                        current_connections.add(conn_tuple)
                        
                        # New connection detected
                        if conn_tuple not in baseline_connections:
                            self.monitoring_data['network_connections'].append({
                                'timestamp': time.time(),
                                'local_addr': conn.laddr,
                                'remote_addr': conn.raddr,
                                'status': conn.status,
                                'pid': conn.pid
                            })
                            
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            baseline_connections = current_connections
            time.sleep(2)
    
    def monitor_registry(self):
        """Monitor registry changes (Windows)"""
        # Monitor common malware registry locations
        monitored_keys = [
            (winreg.HKEY_LOCAL_MACHINE, "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run"),
            (winreg.HKEY_CURRENT_USER, "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run"),
            (winreg.HKEY_LOCAL_MACHINE, "SYSTEM\\CurrentControlSet\\Services")
        ]
        
        baseline_registry = {}
        
        # Take baseline
        for hkey, subkey in monitored_keys:
            try:
                with winreg.OpenKey(hkey, subkey) as key:
                    values = []
                    i = 0
                    while True:
                        try:
                            name, value, type = winreg.EnumValue(key, i)
                            values.append((name, value, type))
                            i += 1
                        except WindowsError:
                            break
                    baseline_registry[subkey] = values
            except WindowsError:
                pass
        
        while self.monitoring:
            for hkey, subkey in monitored_keys:
                try:
                    with winreg.OpenKey(hkey, subkey) as key:
                        current_values = []
                        i = 0
                        while True:
                            try:
                                name, value, type = winreg.EnumValue(key, i)
                                current_values.append((name, value, type))
                                i += 1
                            except WindowsError:
                                break
                        
                        # Compare with baseline
                        baseline_set = set(baseline_registry.get(subkey, []))
                        current_set = set(current_values)
                        
                        # New registry entries
                        new_entries = current_set - baseline_set
                        for entry in new_entries:
                            self.monitoring_data['registry_changes'].append({
                                'timestamp': time.time(),
                                'action': 'added',
                                'key': subkey,
                                'name': entry[0],
                                'value': entry[1],
                                'type': entry[2]
                            })
                        
                        baseline_registry[subkey] = current_values
                        
                except WindowsError:
                    pass
            
            time.sleep(5)
    
    def execute_malware(self):
        """Execute malware sample"""
        try:
            # Start monitoring threads
            self.monitoring = True
            
            process_thread = Thread(target=self.monitor_processes)
            network_thread = Thread(target=self.monitor_network)
            registry_thread = Thread(target=self.monitor_registry)
            
            process_thread.start()
            network_thread.start()
            registry_thread.start()
            
            # Execute malware
            malware_process = subprocess.Popen(
                self.malware_path,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # Wait for analysis duration
            time.sleep(self.analysis_duration)
            
            # Stop monitoring
            self.monitoring = False
            
            # Terminate malware if still running
            try:
                malware_process.terminate()
            except:
                pass
            
            # Wait for monitoring threads to finish
            process_thread.join()
            network_thread.join()
            registry_thread.join()
            
        except Exception as e:
            self.monitoring_data['execution_error'] = str(e)
    
    def run_analysis(self):
        """Run complete dynamic analysis"""
        print("Taking system baseline...")
        self.take_baseline()
        
        print("Starting malware execution and monitoring...")
        self.execute_malware()
        
        print("Analysis complete. Generating report...")
        return self.monitoring_data
    
    def save_report(self, output_file):
        """Save analysis report to file"""
        with open(output_file, 'w') as f:
            json.dump(self.monitoring_data, f, indent=2, default=str)

# Usage example
analyzer = DynamicAnalyzer("/path/to/malware.exe", analysis_duration=300)
results = analyzer.run_analysis()
analyzer.save_report("dynamic_analysis_report.json")
```

## ðŸ› ï¸ UygulamalÄ± Laboratuvar Kurulumu

### ðŸ–¥ï¸ Malware Analysis Lab Setup

```bash
#!/bin/bash
# Malware Analysis Lab Setup Script

echo "Setting up Malware Analysis Laboratory..."

# Create lab directory structure
mkdir -p ~/malware-lab/{samples,tools,reports,vm-snapshots}
cd ~/malware-lab

# Download and setup analysis tools
echo "Installing analysis tools..."

# Static analysis tools
wget https://github.com/NationalSecurityAgency/ghidra/releases/download/Ghidra_10.4_build/ghidra_10.4_PUBLIC_20230928.zip
unzip ghidra_10.4_PUBLIC_20230928.zip -d tools/

# Install Python analysis libraries
pip3 install pefile yara-python ssdeep capstone-engine
pip3 install volatility3 rekall-core
pip3 install oletools python-magic

# Download YARA rules
git clone https://github.com/Yara-Rules/rules.git tools/yara-rules

# Setup Cuckoo Sandbox
echo "Setting up Cuckoo Sandbox..."
pip3 install cuckoo
cuckoo init

# Download sample malware (safe samples for training)
echo "Downloading training samples..."
mkdir samples/training
wget https://github.com/ytisf/theZoo/archive/master.zip -O samples/theZoo.zip

# Setup VM environment
echo "VM setup instructions:"
echo "1. Install VirtualBox or VMware"
echo "2. Create Windows 10 VM with 4GB RAM"
echo "3. Install analysis tools in VM"
echo "4. Take clean snapshot"
echo "5. Configure network isolation"

# Create analysis scripts
cat > tools/quick_analysis.py << 'EOF'
#!/usr/bin/env python3
# Quick malware analysis script

import sys
import os
import hashlib
import pefile
import yara

def quick_analysis(file_path):
    print(f"Analyzing: {file_path}")
    
    # File hashes
    with open(file_path, 'rb') as f:
        data = f.read()
    
    print(f"MD5: {hashlib.md5(data).hexdigest()}")
    print(f"SHA256: {hashlib.sha256(data).hexdigest()}")
    print(f"Size: {len(data)} bytes")
    
    # PE analysis
    try:
        pe = pefile.PE(file_path)
        print(f"Entry Point: {hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint)}")
        print(f"Sections: {len(pe.sections)}")
        
        # Check for suspicious characteristics
        if pe.OPTIONAL_HEADER.AddressOfEntryPoint == 0:
            print("WARNING: Entry point is 0 - possible packed malware")
        
        # High entropy sections
        for section in pe.sections:
            entropy = section.get_entropy()
            if entropy > 7.0:
                print(f"WARNING: High entropy section {section.Name.decode().rstrip(chr(0))}: {entropy}")
                
    except Exception as e:
        print(f"PE analysis failed: {e}")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python3 quick_analysis.py <malware_file>")
        sys.exit(1)
    
    quick_analysis(sys.argv[1])
EOF

chmod +x tools/quick_analysis.py

echo "Malware Analysis Lab setup complete!"
echo "Lab directory: ~/malware-lab"
echo "Tools installed in: ~/malware-lab/tools"
echo "Sample storage: ~/malware-lab/samples"
```

### ðŸ”§ Advanced Analysis Tools Configuration

```python
# Advanced Malware Analysis Toolkit
import os
import subprocess
import json
from datetime import datetime
import volatility3.framework.automagic as automagic
from volatility3.framework import contexts, exceptions
from volatility3.framework.configuration import requirements
from volatility3.framework.plugins.windows import pslist, netscan, malfind

class AdvancedMalwareAnalyzer:
    def __init__(self, sample_path, memory_dump=None):
        self.sample_path = sample_path
        self.memory_dump = memory_dump
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'sample_path': sample_path,
            'static_analysis': {},
            'dynamic_analysis': {},
            'memory_analysis': {},
            'network_analysis': {},
            'behavioral_analysis': {}
        }
    
    def static_analysis(self):
        """Comprehensive static analysis"""
        print("[+] Starting static analysis...")
        
        # File type detection
        file_type = subprocess.run(
            ['file', self.sample_path],
            capture_output=True, text=True
        ).stdout.strip()
        
        self.analysis_results['static_analysis']['file_type'] = file_type
        
        # Entropy analysis
        entropy_result = self.calculate_entropy()
        self.analysis_results['static_analysis']['entropy'] = entropy_result
        
        # String extraction
        strings_result = self.extract_strings()
        self.analysis_results['static_analysis']['strings'] = strings_result
        
        # PE analysis (if applicable)
        if 'PE32' in file_type:
            pe_result = self.analyze_pe_structure()
            self.analysis_results['static_analysis']['pe_analysis'] = pe_result
        
        # YARA scanning
        yara_result = self.yara_scan()
        self.analysis_results['static_analysis']['yara_matches'] = yara_result
        
        return self.analysis_results['static_analysis']
    
    def calculate_entropy(self):
        """Calculate file entropy"""
        import math
        from collections import Counter
        
        with open(self.sample_path, 'rb') as f:
            data = f.read()
        
        # Calculate byte frequency
        byte_counts = Counter(data)
        file_size = len(data)
        
        # Calculate entropy
        entropy = 0
        for count in byte_counts.values():
            probability = count / file_size
            entropy -= probability * math.log2(probability)
        
        return {
            'entropy': entropy,
            'assessment': 'High' if entropy > 7.0 else 'Normal',
            'packed_likelihood': 'High' if entropy > 7.5 else 'Low'
        }
    
    def extract_strings(self, min_length=4):
        """Extract ASCII and Unicode strings"""
        import re
        
        with open(self.sample_path, 'rb') as f:
            data = f.read()
        
        # ASCII strings
        ascii_strings = re.findall(rb'[\x20-\x7E]{' + str(min_length).encode() + rb',}', data)
        
        # Unicode strings
        unicode_strings = re.findall(rb'(?:[\x20-\x7E]\x00){' + str(min_length).encode() + rb',}', data)
        
        # Interesting patterns
        interesting = {
            'urls': re.findall(rb'https?://[\x20-\x7E]+', data),
            'ips': re.findall(rb'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', data),
            'emails': re.findall(rb'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', data),
            'file_paths': re.findall(rb'[A-Za-z]:\\\\[\x20-\x7E\\\\]+', data),
            'registry_keys': re.findall(rb'HKEY_[A-Z_]+\\\\[\x20-\x7E\\\\]+', data)
        }
        
        return {
            'ascii_count': len(ascii_strings),
            'unicode_count': len(unicode_strings),
            'interesting_patterns': {k: [s.decode('utf-8', errors='ignore') for s in v] for k, v in interesting.items()}
        }
    
    def analyze_pe_structure(self):
        """Detailed PE structure analysis"""
        import pefile
        
        try:
            pe = pefile.PE(self.sample_path)
            
            # Basic PE info
            pe_info = {
                'machine_type': hex(pe.FILE_HEADER.Machine),
                'timestamp': pe.FILE_HEADER.TimeDateStamp,
                'entry_point': hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint),
                'image_base': hex(pe.OPTIONAL_HEADER.ImageBase),
                'section_count': len(pe.sections)
            }
            
            # Section analysis
            sections = []
            for section in pe.sections:
                section_info = {
                    'name': section.Name.decode().rstrip('\x00'),
                    'virtual_address': hex(section.VirtualAddress),
                    'virtual_size': section.Misc_VirtualSize,
                    'raw_size': section.SizeOfRawData,
                    'entropy': section.get_entropy(),
                    'characteristics': hex(section.Characteristics)
                }
                
                # Suspicious characteristics
                if section.get_entropy() > 7.0:
                    section_info['suspicious'] = 'High entropy - possibly packed'
                
                if section.Characteristics & 0x20000000:  # IMAGE_SCN_MEM_EXECUTE
                    if section.Characteristics & 0x80000000:  # IMAGE_SCN_MEM_WRITE
                        section_info['suspicious'] = 'Executable and writable'
                
                sections.append(section_info)
            
            pe_info['sections'] = sections
            
            # Import analysis
            imports = []
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_info = {
                        'dll': entry.dll.decode(),
                        'functions': []
                    }
                    
                    for imp in entry.imports:
                        if imp.name:
                            dll_info['functions'].append(imp.name.decode())
                    
                    imports.append(dll_info)
            
            pe_info['imports'] = imports
            
            # Suspicious API detection
            suspicious_apis = [
                'CreateProcess', 'WriteProcessMemory', 'VirtualAlloc',
                'SetWindowsHook', 'RegSetValue', 'CreateService',
                'InternetOpen', 'HttpSendRequest', 'CryptEncrypt'
            ]
            
            found_suspicious = []
            for dll_info in imports:
                for func in dll_info['functions']:
                    if any(api in func for api in suspicious_apis):
                        found_suspicious.append(f"{dll_info['dll']}:{func}")
            
            pe_info['suspicious_apis'] = found_suspicious
            
            return pe_info
            
        except Exception as e:
            return {'error': str(e)}
    
    def yara_scan(self, rules_path='tools/yara-rules'):
        """Scan with YARA rules"""
        import yara
        
        matches = []
        
        # Scan with multiple rule files
        rule_files = [
            'malware/APT_APT1.yar',
            'malware/APT_Lazarus.yar',
            'malware/MALW_Emotet.yar',
            'malware/MALW_TrickBot.yar'
        ]
        
        for rule_file in rule_files:
            rule_path = os.path.join(rules_path, rule_file)
            if os.path.exists(rule_path):
                try:
                    rules = yara.compile(filepath=rule_path)
                    rule_matches = rules.match(self.sample_path)
                    
                    for match in rule_matches:
                        matches.append({
                            'rule_file': rule_file,
                            'rule_name': match.rule,
                            'tags': match.tags,
                            'meta': match.meta
                        })
                        
                except Exception as e:
                    continue
        
        return matches
    
    def memory_analysis(self):
        """Analyze memory dump with Volatility"""
        if not self.memory_dump:
            return {'error': 'No memory dump provided'}
        
        print("[+] Starting memory analysis...")
        
        try:
            # Initialize Volatility context
            ctx = contexts.Context()
            
            # Auto-detect memory dump format
            automagics = automagic.available(ctx)
            
            # Run pslist plugin
            pslist_plugin = pslist.PsList(ctx, config_path="", progress_callback=None)
            
            # This is a simplified example - full implementation would require
            # proper Volatility 3 framework setup
            
            return {
                'status': 'Memory analysis requires full Volatility setup',
                'recommendation': 'Use vol.py -f memory.dump windows.pslist'
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def network_analysis(self):
        """Analyze network behavior"""
        print("[+] Starting network analysis...")
        
        # This would typically involve:
        # 1. Packet capture during dynamic analysis
        # 2. DNS request monitoring
        # 3. HTTP/HTTPS traffic analysis
        # 4. C&C communication detection
        
        return {
            'status': 'Network analysis requires dynamic execution environment',
            'recommendation': 'Use Wireshark or tcpdump during dynamic analysis'
        }
    
    def behavioral_analysis(self):
        """Analyze malware behavior patterns"""
        print("[+] Starting behavioral analysis...")
        
        # Behavioral indicators based on static analysis
        behaviors = []
        
        # Check for persistence mechanisms
        static_results = self.analysis_results.get('static_analysis', {})
        strings = static_results.get('strings', {}).get('interesting_patterns', {})
        
        if strings.get('registry_keys'):
            behaviors.append({
                'category': 'Persistence',
                'description': 'Registry modification detected',
                'evidence': strings['registry_keys'][:5]  # First 5 entries
            })
        
        if strings.get('file_paths'):
            behaviors.append({
                'category': 'File System',
                'description': 'File system access detected',
                'evidence': strings['file_paths'][:5]
            })
        
        if strings.get('urls') or strings.get('ips'):
            behaviors.append({
                'category': 'Network Communication',
                'description': 'Network communication capability detected',
                'evidence': (strings.get('urls', []) + strings.get('ips', []))[:5]
            })
        
        # Check for suspicious APIs
        pe_analysis = static_results.get('pe_analysis', {})
        if pe_analysis.get('suspicious_apis'):
            behaviors.append({
                'category': 'Suspicious API Usage',
                'description': 'Potentially malicious API calls detected',
                'evidence': pe_analysis['suspicious_apis'][:10]
            })
        
        return behaviors
    
    def generate_comprehensive_report(self):
        """Generate comprehensive analysis report"""
        print("[+] Generating comprehensive malware analysis report...")
        
        # Run all analysis modules
        self.static_analysis()
        self.memory_analysis()
        self.network_analysis()
        
        # Behavioral analysis
        self.analysis_results['behavioral_analysis'] = self.behavioral_analysis()
        
        # Risk assessment
        risk_score = self.calculate_risk_score()
        self.analysis_results['risk_assessment'] = risk_score
        
        # Recommendations
        recommendations = self.generate_recommendations()
        self.analysis_results['recommendations'] = recommendations
        
        return self.analysis_results
    
    def calculate_risk_score(self):
        """Calculate overall risk score"""
        score = 0
        factors = []
        
        static_analysis = self.analysis_results.get('static_analysis', {})
        
        # High entropy
        entropy = static_analysis.get('entropy', {})
        if entropy.get('entropy', 0) > 7.0:
            score += 30
            factors.append('High entropy (possibly packed)')
        
        # Suspicious APIs
        pe_analysis = static_analysis.get('pe_analysis', {})
        suspicious_apis = pe_analysis.get('suspicious_apis', [])
        if suspicious_apis:
            score += len(suspicious_apis) * 5
            factors.append(f'{len(suspicious_apis)} suspicious API calls')
        
        # YARA matches
        yara_matches = static_analysis.get('yara_matches', [])
        if yara_matches:
            score += len(yara_matches) * 20
            factors.append(f'{len(yara_matches)} YARA rule matches')
        
        # Network indicators
        strings = static_analysis.get('strings', {}).get('interesting_patterns', {})
        network_indicators = len(strings.get('urls', [])) + len(strings.get('ips', []))
        if network_indicators > 0:
            score += network_indicators * 10
            factors.append(f'{network_indicators} network indicators')
        
        # Determine risk level
        if score >= 80:
            risk_level = 'Critical'
        elif score >= 60:
            risk_level = 'High'
        elif score >= 40:
            risk_level = 'Medium'
        elif score >= 20:
            risk_level = 'Low'
        else:
            risk_level = 'Minimal'
        
        return {
            'score': score,
            'level': risk_level,
            'factors': factors
        }
    
    def generate_recommendations(self):
        """Generate security recommendations"""
        recommendations = []
        
        risk_assessment = self.analysis_results.get('risk_assessment', {})
        risk_level = risk_assessment.get('level', 'Unknown')
        
        if risk_level in ['Critical', 'High']:
            recommendations.extend([
                'Immediately isolate affected systems',
                'Run full antivirus scan on all systems',
                'Check for lateral movement indicators',
                'Review network logs for C&C communication',
                'Consider incident response procedures'
            ])
        
        elif risk_level == 'Medium':
            recommendations.extend([
                'Monitor system for suspicious activity',
                'Update antivirus signatures',
                'Review system logs',
                'Consider additional analysis'
            ])
        
        else:
            recommendations.extend([
                'Continue monitoring',
                'Maintain current security posture',
                'Regular security updates'
            ])
        
        # Specific recommendations based on findings
        static_analysis = self.analysis_results.get('static_analysis', {})
        
        if static_analysis.get('entropy', {}).get('entropy', 0) > 7.0:
            recommendations.append('Consider unpacking analysis for packed malware')
        
        yara_matches = static_analysis.get('yara_matches', [])
        if yara_matches:
            recommendations.append('Review YARA matches for specific threat family indicators')
        
        return recommendations
    
    def save_report(self, output_file):
        """Save analysis report to file"""
        with open(output_file, 'w') as f:
            json.dump(self.analysis_results, f, indent=2, default=str)
        
        print(f"[+] Report saved to: {output_file}")

# Usage example
if __name__ == "__main__":
    analyzer = AdvancedMalwareAnalyzer("/path/to/malware.exe")
    results = analyzer.generate_comprehensive_report()
    analyzer.save_report("comprehensive_malware_report.json")
    
    print(f"Risk Level: {results['risk_assessment']['level']}")
    print(f"Risk Score: {results['risk_assessment']['score']}")
```

## ðŸŽ¯ Pratik Egzersizler

### Egzersiz 1: Packed Malware Analysis

```python
# Packed Malware Unpacking Exercise
import pefile
import struct
import os

class PackedMalwareAnalyzer:
    def __init__(self, packed_file):
        self.packed_file = packed_file
        self.pe = pefile.PE(packed_file)
        
    def detect_packer(self):
        """Detect common packers"""
        packer_signatures = {
            'UPX': [b'UPX0', b'UPX1', b'UPX!'],
            'ASPack': [b'aPLib'],
            'PECompact': [b'PECompact'],
            'Themida': [b'Themida'],
            'VMProtect': [b'VMProtect']
        }
        
        with open(self.packed_file, 'rb') as f:
            data = f.read()
        
        detected_packers = []
        for packer, signatures in packer_signatures.items():
            for sig in signatures:
                if sig in data:
                    detected_packers.append(packer)
                    break
        
        return detected_packers
    
    def analyze_entropy_distribution(self):
        """Analyze entropy distribution across sections"""
        import math
        from collections import Counter
        
        section_entropies = []
        
        for section in self.pe.sections:
            section_data = section.get_data()
            if len(section_data) > 0:
                # Calculate entropy
                byte_counts = Counter(section_data)
                entropy = 0
                for count in byte_counts.values():
                    probability = count / len(section_data)
                    entropy -= probability * math.log2(probability)
                
                section_entropies.append({
                    'name': section.Name.decode().rstrip('\x00'),
                    'entropy': entropy,
                    'size': len(section_data),
                    'packed_likelihood': 'High' if entropy > 7.0 else 'Low'
                })
        
        return section_entropies
    
    def find_oep_candidates(self):
        """Find Original Entry Point candidates"""
        # Look for common unpacking stubs
        entry_point = self.pe.OPTIONAL_HEADER.AddressOfEntryPoint
        
        # Get code at entry point
        ep_data = self.pe.get_data(entry_point, 100)
        
        # Common unpacking patterns
        unpacking_patterns = [
            b'\x60\x8B\xEC',  # PUSHAD; MOV EBP, ESP
            b'\x55\x8B\xEC',  # PUSH EBP; MOV EBP, ESP
            b'\x53\x56\x57',  # PUSH EBX; PUSH ESI; PUSH EDI
        ]
        
        candidates = []
        for i, pattern in enumerate(unpacking_patterns):
            if pattern in ep_data:
                candidates.append({
                    'pattern': pattern.hex(),
                    'description': f'Unpacking pattern {i+1}',
                    'offset': ep_data.find(pattern)
                })
        
        return candidates
    
    def extract_embedded_pe(self):
        """Extract embedded PE files"""
        with open(self.packed_file, 'rb') as f:
            data = f.read()
        
        # Look for PE signatures
        pe_signatures = [b'MZ', b'PE\x00\x00']
        embedded_pes = []
        
        for sig in pe_signatures:
            offset = 0
            while True:
                offset = data.find(sig, offset + 1)
                if offset == -1:
                    break
                
                # Verify if it's a valid PE
                try:
                    if sig == b'MZ':
                        # Check for PE signature
                        pe_offset = struct.unpack('<L', data[offset+60:offset+64])[0]
                        if data[offset+pe_offset:offset+pe_offset+4] == b'PE\x00\x00':
                            embedded_pes.append({
                                'offset': offset,
                                'type': 'PE file',
                                'size_estimate': 'Unknown'
                            })
                except:
                    continue
        
        return embedded_pes

# Usage example
analyzer = PackedMalwareAnalyzer("/path/to/packed_malware.exe")
packers = analyzer.detect_packer()
entropy = analyzer.analyze_entropy_distribution()
oep_candidates = analyzer.find_oep_candidates()
embedded = analyzer.extract_embedded_pe()

print(f"Detected packers: {packers}")
print(f"High entropy sections: {[s['name'] for s in entropy if s['entropy'] > 7.0]}")
```

### Egzersiz 2: Memory Forensics with Volatility

```python
# Memory Forensics Analysis
import subprocess
import json
import os

class MemoryForensicsAnalyzer:
    def __init__(self, memory_dump_path):
        self.memory_dump = memory_dump_path
        self.volatility_path = "vol.py"  # Adjust path as needed
        
    def run_volatility_command(self, plugin, additional_args=""):
        """Run Volatility command and return output"""
        cmd = f"{self.volatility_path} -f {self.memory_dump} {plugin} {additional_args}"
        
        try:
            result = subprocess.run(
                cmd.split(),
                capture_output=True,
                text=True,
                timeout=300
            )
            return result.stdout
        except subprocess.TimeoutExpired:
            return "Command timed out"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def analyze_processes(self):
        """Analyze running processes"""
        print("[+] Analyzing processes...")
        
        # Get process list
        pslist_output = self.run_volatility_command("windows.pslist")
        
        # Get process tree
        pstree_output = self.run_volatility_command("windows.pstree")
        
        # Find hidden processes
        psxview_output = self.run_volatility_command("windows.psxview")
        
        return {
            'process_list': pslist_output,
            'process_tree': pstree_output,
            'hidden_processes': psxview_output
        }
    
    def analyze_network_connections(self):
        """Analyze network connections"""
        print("[+] Analyzing network connections...")
        
        # Network connections
        netscan_output = self.run_volatility_command("windows.netscan")
        
        # Network statistics
        netstat_output = self.run_volatility_command("windows.netstat")
        
        return {
            'network_scan': netscan_output,
            'network_stats': netstat_output
        }
    
    def find_malware_artifacts(self):
        """Find malware artifacts in memory"""
        print("[+] Searching for malware artifacts...")
        
        # Malware detection
        malfind_output = self.run_volatility_command("windows.malfind")
        
        # Suspicious processes
        hollowfind_output = self.run_volatility_command("windows.hollowfind")
        
        # Injected code
        ldrmodules_output = self.run_volatility_command("windows.ldrmodules")
        
        return {
            'malfind': malfind_output,
            'hollow_processes': hollowfind_output,
            'module_anomalies': ldrmodules_output
        }
    
    def extract_suspicious_processes(self, pid_list):
        """Extract suspicious processes for analysis"""
        print(f"[+] Extracting processes: {pid_list}")
        
        extracted_files = []
        
        for pid in pid_list:
            # Dump process
            output_file = f"process_{pid}.dmp"
            dump_output = self.run_volatility_command(
                "windows.pslist",
                f"--pid {pid} --dump"
            )
            
            if os.path.exists(output_file):
                extracted_files.append(output_file)
        
        return extracted_files
    
    def analyze_registry(self):
        """Analyze registry for persistence mechanisms"""
        print("[+] Analyzing registry...")
        
        # Print registry keys
        registry_output = self.run_volatility_command("windows.registry.printkey")
        
        # Specific persistence locations
        persistence_keys = [
            "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run",
            "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce",
            "SYSTEM\\CurrentControlSet\\Services"
        ]
        
        persistence_analysis = {}
        for key in persistence_keys:
            key_output = self.run_volatility_command(
                "windows.registry.printkey",
                f"--key '{key}'"
            )
            persistence_analysis[key] = key_output
        
        return {
            'full_registry': registry_output,
            'persistence_keys': persistence_analysis
        }
    
    def comprehensive_analysis(self):
        """Run comprehensive memory analysis"""
        analysis_results = {
            'memory_dump': self.memory_dump,
            'processes': self.analyze_processes(),
            'network': self.analyze_network_connections(),
            'malware_artifacts': self.find_malware_artifacts(),
            'registry': self.analyze_registry()
        }
        
        return analysis_results
    
    def generate_report(self, output_file):
        """Generate comprehensive memory forensics report"""
        results = self.comprehensive_analysis()
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"[+] Memory forensics report saved to: {output_file}")
        return results

# Usage example
analyzer = MemoryForensicsAnalyzer("/path/to/memory.dump")
results = analyzer.generate_report("memory_forensics_report.json")
```

### Egzersiz 3: Automated Malware Classification

```python
# Automated Malware Classification System
import os
import json
import hashlib
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import pefile

class MalwareClassifier:
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.feature_names = []
        self.is_trained = False
        
    def extract_features(self, file_path):
        """Extract features from malware sample"""
        features = {}
        
        try:
            # File-based features
            with open(file_path, 'rb') as f:
                data = f.read()
            
            features['file_size'] = len(data)
            features['entropy'] = self.calculate_entropy(data)
            
            # PE-specific features
            if self.is_pe_file(file_path):
                pe_features = self.extract_pe_features(file_path)
                features.update(pe_features)
            
            # String features
            string_features = self.extract_string_features(data)
            features.update(string_features)
            
            # Byte histogram features
            byte_features = self.extract_byte_histogram(data)
            features.update(byte_features)
            
        except Exception as e:
            print(f"Error extracting features from {file_path}: {e}")
            return None
        
        return features
    
    def calculate_entropy(self, data):
        """Calculate Shannon entropy"""
        import math
        from collections import Counter
        
        if len(data) == 0:
            return 0
        
        byte_counts = Counter(data)
        entropy = 0
        
        for count in byte_counts.values():
            probability = count / len(data)
            entropy -= probability * math.log2(probability)
        
        return entropy
    
    def is_pe_file(self, file_path):
        """Check if file is PE format"""
        try:
            pefile.PE(file_path)
            return True
        except:
            return False
    
    def extract_pe_features(self, file_path):
        """Extract PE-specific features"""
        features = {}
        
        try:
            pe = pefile.PE(file_path)
            
            # Header features
            features['pe_machine'] = pe.FILE_HEADER.Machine
            features['pe_sections'] = len(pe.sections)
            features['pe_timestamp'] = pe.FILE_HEADER.TimeDateStamp
            features['pe_characteristics'] = pe.FILE_HEADER.Characteristics
            
            # Optional header features
            features['pe_entry_point'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
            features['pe_image_base'] = pe.OPTIONAL_HEADER.ImageBase
            features['pe_size_of_image'] = pe.OPTIONAL_HEADER.SizeOfImage
            
            # Section features
            executable_sections = 0
            writable_sections = 0
            total_entropy = 0
            
            for section in pe.sections:
                if section.Characteristics & 0x20000000:  # IMAGE_SCN_MEM_EXECUTE
                    executable_sections += 1
                if section.Characteristics & 0x80000000:  # IMAGE_SCN_MEM_WRITE
                    writable_sections += 1
                total_entropy += section.get_entropy()
            
            features['pe_executable_sections'] = executable_sections
            features['pe_writable_sections'] = writable_sections
            features['pe_avg_section_entropy'] = total_entropy / len(pe.sections)
            
            # Import features
            import_count = 0
            dll_count = 0
            
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                dll_count = len(pe.DIRECTORY_ENTRY_IMPORT)
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    import_count += len(entry.imports)
            
            features['pe_import_count'] = import_count
            features['pe_dll_count'] = dll_count
            
        except Exception as e:
            print(f"Error extracting PE features: {e}")
        
        return features
    
    def extract_string_features(self, data):
        """Extract string-based features"""
        import re
        
        features = {}
        
        # Extract strings
        ascii_strings = re.findall(b'[\x20-\x7E]{4,}', data)
        unicode_strings = re.findall(b'(?:[\x20-\x7E]\x00){4,}', data)
        
        features['string_count_ascii'] = len(ascii_strings)
        features['string_count_unicode'] = len(unicode_strings)
        
        # Specific pattern counts
        all_strings = b' '.join(ascii_strings + unicode_strings)
        
        features['url_count'] = len(re.findall(b'https?://', all_strings))
        features['ip_count'] = len(re.findall(b'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', all_strings))
        features['email_count'] = len(re.findall(b'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', all_strings))
        features['registry_count'] = len(re.findall(b'HKEY_', all_strings))
        features['file_path_count'] = len(re.findall(b'[A-Za-z]:\\\\', all_strings))
        
        return features
    
    def extract_byte_histogram(self, data, bins=256):
        """Extract byte frequency histogram features"""
        features = {}
        
        # Calculate byte frequency
        byte_counts = [0] * bins
        for byte in data:
            byte_counts[byte] += 1
        
        # Normalize
        total_bytes = len(data)
        if total_bytes > 0:
            byte_frequencies = [count / total_bytes for count in byte_counts]
        else:
            byte_frequencies = [0] * bins
        
        # Add as features
        for i, freq in enumerate(byte_frequencies):
            features[f'byte_freq_{i}'] = freq
        
        return features
    
    def prepare_training_data(self, dataset_path):
        """Prepare training data from labeled dataset"""
        features_list = []
        labels = []
        
        # Expected directory structure:
        # dataset_path/
        #   â”œâ”€â”€ malware/
        #   â”‚   â”œâ”€â”€ trojan/
        #   â”‚   â”œâ”€â”€ virus/
        #   â”‚   â”œâ”€â”€ worm/
        #   â”‚   â””â”€â”€ ransomware/
        #   â””â”€â”€ benign/
        
        malware_types = ['trojan', 'virus', 'worm', 'ransomware']
        
        # Process malware samples
        for malware_type in malware_types:
            type_path = os.path.join(dataset_path, 'malware', malware_type)
            if os.path.exists(type_path):
                for filename in os.listdir(type_path):
                    file_path = os.path.join(type_path, filename)
                    if os.path.isfile(file_path):
                        features = self.extract_features(file_path)
                        if features:
                            features_list.append(features)
                            labels.append(malware_type)
        
        # Process benign samples
        benign_path = os.path.join(dataset_path, 'benign')
        if os.path.exists(benign_path):
            for filename in os.listdir(benign_path):
                file_path = os.path.join(benign_path, filename)
                if os.path.isfile(file_path):
                    features = self.extract_features(file_path)
                    if features:
                        features_list.append(features)
                        labels.append('benign')
        
        return features_list, labels
    
    def train_model(self, dataset_path):
        """Train the malware classification model"""
        print("[+] Preparing training data...")
        features_list, labels = self.prepare_training_data(dataset_path)
        
        if not features_list:
            raise ValueError("No training data found")
        
        print(f"[+] Loaded {len(features_list)} samples")
        
        # Convert to DataFrame for easier handling
        import pandas as pd
        df = pd.DataFrame(features_list)
        df['label'] = labels
        
        # Handle missing values
        df = df.fillna(0)
        
        # Separate features and labels
        X = df.drop('label', axis=1)
        y = df['label']
        
        # Store feature names
        self.feature_names = list(X.columns)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print("[+] Training model...")
        self.model.fit(X_train, y_train)
        
        # Evaluate model
        y_pred = self.model.predict(X_test)
        print("\n[+] Model Performance:")
        print(classification_report(y_test, y_pred))
        
        self.is_trained = True
        return self.model
    
    def classify_sample(self, file_path):
        """Classify a malware sample"""
        if not self.is_trained:
            raise ValueError("Model not trained yet")
        
        # Extract features
        features = self.extract_features(file_path)
        if not features:
            return None
        
        # Convert to DataFrame
        import pandas as pd
        df = pd.DataFrame([features])
        
        # Ensure all required features are present
        for feature_name in self.feature_names:
            if feature_name not in df.columns:
                df[feature_name] = 0
        
        # Select only training features
        X = df[self.feature_names].fillna(0)
        
        # Predict
        prediction = self.model.predict(X)[0]
        probabilities = self.model.predict_proba(X)[0]
        
        # Get class names
        classes = self.model.classes_
        
        # Create result
        result = {
            'prediction': prediction,
            'confidence': max(probabilities),
            'probabilities': dict(zip(classes, probabilities))
        }
        
        return result
    
    def save_model(self, model_path):
        """Save trained model"""
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'is_trained': self.is_trained
        }
        joblib.dump(model_data, model_path)
        print(f"[+] Model saved to: {model_path}")
    
    def load_model(self, model_path):
        """Load trained model"""
        model_data = joblib.load(model_path)
        self.model = model_data['model']
        self.feature_names = model_data['feature_names']
        self.is_trained = model_data['is_trained']
        print(f"[+] Model loaded from: {model_path}")

# Usage example
if __name__ == "__main__":
    classifier = MalwareClassifier()
    
    # Train model (if you have a dataset)
    # classifier.train_model("/path/to/dataset")
    # classifier.save_model("malware_classifier.pkl")
    
    # Load pre-trained model
    # classifier.load_model("malware_classifier.pkl")
    
    # Classify a sample
    # result = classifier.classify_sample("/path/to/unknown_sample.exe")
    # print(f"Classification: {result['prediction']} (confidence: {result['confidence']:.2f})")
```

## ðŸ”§ Ã–nerilen AraÃ§lar

### ðŸ› ï¸ Ticari AraÃ§lar

#### IDA Pro
- **Ã–zellikler**: Industry standard disassembler
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Advanced analysis capabilities, plugin ecosystem
- **KullanÄ±m AlanlarÄ±**: Reverse engineering, vulnerability research
- **Lisans**: Commercial ($$$)

#### Hex-Rays Decompiler
- **Ã–zellikler**: High-level code reconstruction
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: C-like pseudocode generation
- **KullanÄ±m AlanlarÄ±**: Complex malware analysis
- **Lisans**: Commercial ($$$$)

#### VMware Workstation/vSphere
- **Ã–zellikler**: Professional virtualization
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Snapshot management, network isolation
- **KullanÄ±m AlanlarÄ±**: Safe malware execution environment
- **Lisans**: Commercial ($$)

### ðŸ†“ AÃ§Ä±k Kaynak AraÃ§lar

#### Ghidra
- **Ã–zellikler**: NSA's reverse engineering framework
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Free, powerful decompiler
- **KullanÄ±m AlanlarÄ±**: Static analysis, reverse engineering
- **Lisans**: Open source

```bash
# Ghidra installation
wget https://github.com/NationalSecurityAgency/ghidra/releases/download/Ghidra_10.4_build/ghidra_10.4_PUBLIC_20230928.zip
unzip ghidra_10.4_PUBLIC_20230928.zip
cd ghidra_10.4_PUBLIC
./ghidraRun
```

#### x64dbg
- **Ã–zellikler**: Windows debugger
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: User-friendly interface, plugin support
- **KullanÄ±m AlanlarÄ±**: Dynamic analysis, debugging
- **Lisans**: Open source

#### YARA
- **Ã–zellikler**: Pattern matching engine
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Flexible rule creation
- **KullanÄ±m AlanlarÄ±**: Malware detection, classification
- **Lisans**: Open source

```bash
# YARA installation
sudo apt-get install yara
# or
brew install yara
```

#### Volatility
- **Ã–zellikler**: Memory forensics framework
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Comprehensive memory analysis
- **KullanÄ±m AlanlarÄ±**: Memory dump analysis
- **Lisans**: Open source

```bash
# Volatility 3 installation
pip3 install volatility3
```

#### Cuckoo Sandbox
- **Ã–zellikler**: Automated malware analysis
- **GÃ¼Ã§lÃ¼ YanlarÄ±**: Behavioral analysis, reporting
- **KullanÄ±m AlanlarÄ±**: Dynamic analysis automation
- **Lisans**: Open source

```bash
# Cuckoo installation
pip3 install cuckoo
cuckoo init
```

## ðŸ“‹ YapÄ±landÄ±rma En Ä°yi UygulamalarÄ±

### ðŸ”’ GÃ¼venli Analiz OrtamÄ±

```bash
#!/bin/bash
# Secure Analysis Environment Setup

echo "Setting up secure malware analysis environment..."

# Create isolated network
sudo iptables -A OUTPUT -j DROP
sudo iptables -A INPUT -j DROP
sudo iptables -A OUTPUT -o lo -j ACCEPT
sudo iptables -A INPUT -i lo -j ACCEPT

# Allow specific analysis traffic only
sudo iptables -A OUTPUT -p tcp --dport 80 -j ACCEPT
sudo iptables -A OUTPUT -p tcp --dport 443 -j ACCEPT
sudo iptables -A OUTPUT -p tcp --dport 53 -j ACCEPT
sudo iptables -A OUTPUT -p udp --dport 53 -j ACCEPT

# Log all network activity
sudo iptables -A OUTPUT -j LOG --log-prefix "MALWARE_NET: "

# Setup monitoring
sudo tcpdump -i any -w malware_traffic.pcap &

# Create analysis user with limited privileges
sudo useradd -m -s /bin/bash malware_analyst
sudo usermod -aG sudo malware_analyst

# Setup chroot environment
sudo mkdir -p /chroot/malware_analysis
sudo debootstrap stable /chroot/malware_analysis

echo "Secure environment setup complete!"
```

### ðŸ–¥ï¸ VM Snapshot Management

```python
# VM Snapshot Management Script
import subprocess
import datetime
import json

class VMSnapshotManager:
    def __init__(self, vm_name, hypervisor='virtualbox'):
        self.vm_name = vm_name
        self.hypervisor = hypervisor
        self.snapshots = []
        
    def create_snapshot(self, snapshot_name=None):
        """Create VM snapshot"""
        if not snapshot_name:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            snapshot_name = f"analysis_{timestamp}"
        
        if self.hypervisor == 'virtualbox':
            cmd = f"VBoxManage snapshot {self.vm_name} take {snapshot_name}"
        elif self.hypervisor == 'vmware':
            cmd = f"vmrun snapshot {self.vm_name} {snapshot_name}"
        
        try:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode == 0:
                self.snapshots.append({
                    'name': snapshot_name,
                    'created': datetime.datetime.now().isoformat(),
                    'status': 'success'
                })
                print(f"[+] Snapshot created: {snapshot_name}")
                return True
            else:
                print(f"[-] Snapshot creation failed: {result.stderr}")
                return False
        except Exception as e:
            print(f"[-] Error creating snapshot: {e}")
            return False
    
    def restore_snapshot(self, snapshot_name):
        """Restore VM to snapshot"""
        if self.hypervisor == 'virtualbox':
            cmd = f"VBoxManage snapshot {self.vm_name} restore {snapshot_name}"
        elif self.hypervisor == 'vmware':
            cmd = f"vmrun revertToSnapshot {self.vm_name} {snapshot_name}"
        
        try:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode == 0:
                print(f"[+] Restored to snapshot: {snapshot_name}")
                return True
            else:
                print(f"[-] Restore failed: {result.stderr}")
                return False
        except Exception as e:
            print(f"[-] Error restoring snapshot: {e}")
            return False
    
    def list_snapshots(self):
        """List available snapshots"""
        if self.hypervisor == 'virtualbox':
            cmd = f"VBoxManage snapshot {self.vm_name} list"
        elif self.hypervisor == 'vmware':
            cmd = f"vmrun listSnapshots {self.vm_name}"
        
        try:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode == 0:
                print(f"[+] Available snapshots:\n{result.stdout}")
                return result.stdout
            else:
                print(f"[-] Failed to list snapshots: {result.stderr}")
                return None
        except Exception as e:
            print(f"[-] Error listing snapshots: {e}")
            return None
    
    def delete_snapshot(self, snapshot_name):
        """Delete snapshot"""
        if self.hypervisor == 'virtualbox':
            cmd = f"VBoxManage snapshot {self.vm_name} delete {snapshot_name}"
        elif self.hypervisor == 'vmware':
            cmd = f"vmrun deleteSnapshot {self.vm_name} {snapshot_name}"
        
        try:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode == 0:
                print(f"[+] Snapshot deleted: {snapshot_name}")
                return True
            else:
                print(f"[-] Delete failed: {result.stderr}")
                return False
        except Exception as e:
            print(f"[-] Error deleting snapshot: {e}")
            return False

# Usage example
vm_manager = VMSnapshotManager("MalwareAnalysisVM")
vm_manager.create_snapshot("clean_state")
# ... run malware analysis ...
vm_manager.restore_snapshot("clean_state")
```

## ðŸŒ GerÃ§ek DÃ¼nya Vaka Ã‡alÄ±ÅŸmalarÄ±

### ðŸ“Š Vaka 1: APT Malware Analizi

**Senaryo**: Åžirket aÄŸÄ±nda tespit edilen ÅŸÃ¼pheli dosya analizi

**Analiz SÃ¼reci**:
1. **Ä°lk Triage**: Dosya hash kontrolÃ¼, VirusTotal sorgusu
2. **Statik Analiz**: PE header analizi, string extraction
3. **Dinamik Analiz**: Sandbox execution, network monitoring
4. **Tersine MÃ¼hendislik**: IDA Pro ile kod analizi
5. **IOC Extraction**: Network indicators, file artifacts

**Bulgular**:
- C2 sunucu adresleri
- Persistence mekanizmalarÄ±
- Data exfiltration yÃ¶ntemleri
- Attribution indicators

### ðŸ“Š Vaka 2: Ransomware Ailesi Analizi

**Senaryo**: Yeni ransomware varyantÄ±nÄ±n analizi

**Analiz AdÄ±mlarÄ±**:
1. **Encryption Analysis**: KullanÄ±lan ÅŸifreleme algoritmasÄ±
2. **Key Management**: Anahtar Ã¼retimi ve saklama
3. **Payment Mechanism**: Bitcoin wallet analizi
4. **Decryption Possibility**: ZayÄ±flÄ±k araÅŸtÄ±rmasÄ±

### ðŸ“Š Vaka 3: Banking Trojan Analizi

**Senaryo**: Online banking hedefleyen malware

**Teknik Detaylar**:
- Web injection teknikleri
- Certificate pinning bypass
- Two-factor authentication bypass
- Transaction manipulation

## â“ Bilgi Kontrol SorularÄ±

### ðŸ“ Teorik Sorular

1. **Statik vs Dinamik Analiz**:
   - Statik analizin avantajlarÄ± ve dezavantajlarÄ± nelerdir?
   - Hangi durumlarda dinamik analiz tercih edilir?

2. **Anti-Analysis Teknikleri**:
   - Packing ve obfuscation arasÄ±ndaki fark nedir?
   - VM detection teknikleri nasÄ±l Ã§alÄ±ÅŸÄ±r?

3. **Malware SÄ±nÄ±flandÄ±rmasÄ±**:
   - Trojan ve virus arasÄ±ndaki temel farklar nelerdir?
   - APT malware'inin karakteristik Ã¶zellikleri nelerdir?

### ðŸ”§ Pratik Sorular

1. **AraÃ§ KullanÄ±mÄ±**:
   - IDA Pro'da function graph nasÄ±l oluÅŸturulur?
   - Volatility ile hangi memory artifacts analiz edilebilir?

2. **Analiz Teknikleri**:
   - Packed malware nasÄ±l unpack edilir?
   - Network traffic'ten C2 communication nasÄ±l tespit edilir?

## ðŸ“š Pratik Ã–devler

### ðŸŽ¯ Ã–dev 1: Malware Triage Sistemi

**Hedef**: Otomatik malware triage sistemi geliÅŸtirme

**Gereksinimler**:
- Dosya hash kontrolÃ¼
- VirusTotal API entegrasyonu
- Statik analiz Ã¶zellikleri
- Risk skorlama sistemi

**Teslim Edilecekler**:
- Python kodu
- Analiz raporu ÅŸablonu
- Test sonuÃ§larÄ±

### ðŸŽ¯ Ã–dev 2: Memory Forensics Analizi

**Hedef**: Memory dump'tan malware artifacts Ã§Ä±karma

**AraÃ§lar**:
- Volatility Framework
- Custom Python scripts
- Hex editor

**Analiz Hedefleri**:
- Process injection detection
- Hidden processes
- Network connections
- Registry modifications

### ðŸŽ¯ Ã–dev 3: Yara Rule GeliÅŸtirme

**Hedef**: Malware ailesi iÃ§in detection rule yazma

**Kapsam**:
- String-based detection
- Byte pattern matching
- Behavioral indicators
- False positive minimization

## ðŸ“Š Performans Metrikleri

### ðŸŽ¯ Analiz Etkinlik Ã–lÃ§Ã¼mÃ¼

```python
# Malware Analysis Performance Metrics
import time
import json
from datetime import datetime

class AnalysisMetrics:
    def __init__(self):
        self.metrics = {
            'total_samples': 0,
            'analysis_times': [],
            'detection_accuracy': [],
            'false_positives': 0,
            'false_negatives': 0,
            'tool_performance': {}
        }
    
    def start_analysis(self, sample_id):
        """Start timing analysis"""
        self.current_analysis = {
            'sample_id': sample_id,
            'start_time': time.time(),
            'tools_used': []
        }
    
    def end_analysis(self, result):
        """End timing and record results"""
        end_time = time.time()
        analysis_time = end_time - self.current_analysis['start_time']
        
        self.metrics['total_samples'] += 1
        self.metrics['analysis_times'].append(analysis_time)
        
        # Record tool performance
        for tool in self.current_analysis['tools_used']:
            if tool not in self.metrics['tool_performance']:
                self.metrics['tool_performance'][tool] = []
            self.metrics['tool_performance'][tool].append(analysis_time)
    
    def record_detection_result(self, predicted, actual):
        """Record detection accuracy"""
        if predicted == actual:
            accuracy = 1.0
        else:
            accuracy = 0.0
            if predicted == 'malware' and actual == 'benign':
                self.metrics['false_positives'] += 1
            elif predicted == 'benign' and actual == 'malware':
                self.metrics['false_negatives'] += 1
        
        self.metrics['detection_accuracy'].append(accuracy)
    
    def generate_report(self):
        """Generate performance report"""
        if not self.metrics['analysis_times']:
            return "No analysis data available"
        
        avg_time = sum(self.metrics['analysis_times']) / len(self.metrics['analysis_times'])
        avg_accuracy = sum(self.metrics['detection_accuracy']) / len(self.metrics['detection_accuracy']) if self.metrics['detection_accuracy'] else 0
        
        total_predictions = len(self.metrics['detection_accuracy'])
        fp_rate = self.metrics['false_positives'] / total_predictions if total_predictions > 0 else 0
        fn_rate = self.metrics['false_negatives'] / total_predictions if total_predictions > 0 else 0
        
        report = f"""
        Malware Analysis Performance Report
        ===================================
        
        Total Samples Analyzed: {self.metrics['total_samples']}
        Average Analysis Time: {avg_time:.2f} seconds
        Detection Accuracy: {avg_accuracy:.2%}
        False Positive Rate: {fp_rate:.2%}
        False Negative Rate: {fn_rate:.2%}
        
        Tool Performance:
        """
        
        for tool, times in self.metrics['tool_performance'].items():
            avg_tool_time = sum(times) / len(times)
            report += f"\n        {tool}: {avg_tool_time:.2f}s average"
        
        return report
    
    def export_metrics(self, filename):
        """Export metrics to JSON"""
        with open(filename, 'w') as f:
            json.dump(self.metrics, f, indent=2)

# Usage example
metrics = AnalysisMetrics()
metrics.start_analysis("sample_001")
# ... perform analysis ...
metrics.end_analysis("malware_detected")
metrics.record_detection_result("malware", "malware")
print(metrics.generate_report())
```

## ðŸ¤– Yapay Zeka ve Makine Ã–ÄŸrenimi UygulamalarÄ±

### ðŸ§  AI-Powered Malware Detection

```python
# Advanced AI Malware Detection System
import tensorflow as tf
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib

class AIMalwareDetector:
    def __init__(self):
        self.cnn_model = None
        self.anomaly_detector = IsolationForest(contamination=0.1)
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def build_cnn_model(self, input_shape):
        """Build CNN model for malware detection"""
        model = tf.keras.Sequential([
            tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
            tf.keras.layers.MaxPooling1D(2),
            tf.keras.layers.Conv1D(128, 3, activation='relu'),
            tf.keras.layers.MaxPooling1D(2),
            tf.keras.layers.Conv1D(256, 3, activation='relu'),
            tf.keras.layers.GlobalMaxPooling1D(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def preprocess_binary(self, file_path, max_length=100000):
        """Convert binary file to feature vector"""
        try:
            with open(file_path, 'rb') as f:
                binary_data = f.read(max_length)
            
            # Convert to byte array
            byte_array = np.frombuffer(binary_data, dtype=np.uint8)
            
            # Pad or truncate to fixed length
            if len(byte_array) < max_length:
                byte_array = np.pad(byte_array, (0, max_length - len(byte_array)))
            else:
                byte_array = byte_array[:max_length]
            
            # Normalize
            byte_array = byte_array.astype(np.float32) / 255.0
            
            return byte_array.reshape(1, -1, 1)
        
        except Exception as e:
            print(f"Error preprocessing {file_path}: {e}")
            return None
    
    def extract_advanced_features(self, file_path):
        """Extract advanced features for anomaly detection"""
        features = []
        
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            # Entropy calculation
            entropy = self.calculate_entropy(data)
            features.append(entropy)
            
            # Byte frequency analysis
            byte_freq = np.bincount(data, minlength=256) / len(data)
            features.extend(byte_freq)
            
            # N-gram analysis
            bigrams = self.extract_ngrams(data, n=2)
            trigrams = self.extract_ngrams(data, n=3)
            
            # Statistical features
            features.extend([
                np.mean(data),
                np.std(data),
                np.var(data),
                len(data),
                len(set(data))  # unique bytes
            ])
            
            return np.array(features)
        
        except Exception as e:
            print(f"Error extracting features from {file_path}: {e}")
            return None
    
    def calculate_entropy(self, data):
        """Calculate Shannon entropy"""
        if len(data) == 0:
            return 0
        
        # Count byte frequencies
        byte_counts = np.bincount(data)
        probabilities = byte_counts[byte_counts > 0] / len(data)
        
        # Calculate entropy
        entropy = -np.sum(probabilities * np.log2(probabilities))
        return entropy
    
    def extract_ngrams(self, data, n=2):
        """Extract n-gram features"""
        ngrams = {}
        for i in range(len(data) - n + 1):
            ngram = tuple(data[i:i+n])
            ngrams[ngram] = ngrams.get(ngram, 0) + 1
        
        # Return top 100 most frequent n-grams
        sorted_ngrams = sorted(ngrams.items(), key=lambda x: x[1], reverse=True)
        return [count for _, count in sorted_ngrams[:100]]
    
    def train_models(self, train_data, train_labels):
        """Train both CNN and anomaly detection models"""
        print("[+] Training CNN model...")
        
        # Prepare data for CNN
        cnn_data = []
        feature_data = []
        
        for file_path in train_data:
            # CNN features
            cnn_features = self.preprocess_binary(file_path)
            if cnn_features is not None:
                cnn_data.append(cnn_features[0])
            
            # Advanced features for anomaly detection
            adv_features = self.extract_advanced_features(file_path)
            if adv_features is not None:
                feature_data.append(adv_features)
        
        if not cnn_data or not feature_data:
            raise ValueError("No valid training data found")
        
        # Train CNN
        cnn_data = np.array(cnn_data)
        train_labels_np = np.array(train_labels)
        
        input_shape = (cnn_data.shape[1], 1)
        self.cnn_model = self.build_cnn_model(input_shape)
        
        # Train with validation split
        history = self.cnn_model.fit(
            cnn_data, train_labels_np,
            epochs=50,
            batch_size=32,
            validation_split=0.2,
            verbose=1
        )
        
        # Train anomaly detector on benign samples only
        print("[+] Training anomaly detector...")
        benign_features = [feature_data[i] for i, label in enumerate(train_labels) if label == 0]
        
        if benign_features:
            benign_features = np.array(benign_features)
            # Handle variable length features
            max_len = max(len(f) for f in benign_features)
            padded_features = []
            for f in benign_features:
                if len(f) < max_len:
                    f = np.pad(f, (0, max_len - len(f)))
                padded_features.append(f)
            
            benign_features = np.array(padded_features)
            benign_features = self.scaler.fit_transform(benign_features)
            self.anomaly_detector.fit(benign_features)
        
        self.is_trained = True
        print("[+] Training completed!")
        
        return history
    
    def predict(self, file_path):
        """Predict if file is malware using ensemble approach"""
        if not self.is_trained:
            raise ValueError("Models not trained yet")
        
        # CNN prediction
        cnn_features = self.preprocess_binary(file_path)
        if cnn_features is None:
            return None
        
        cnn_prob = self.cnn_model.predict(cnn_features)[0][0]
        
        # Anomaly detection
        adv_features = self.extract_advanced_features(file_path)
        if adv_features is not None:
            # Pad features to match training data
            max_len = self.scaler.n_features_in_
            if len(adv_features) < max_len:
                adv_features = np.pad(adv_features, (0, max_len - len(adv_features)))
            elif len(adv_features) > max_len:
                adv_features = adv_features[:max_len]
            
            adv_features = self.scaler.transform([adv_features])
            anomaly_score = self.anomaly_detector.decision_function(adv_features)[0]
            is_anomaly = self.anomaly_detector.predict(adv_features)[0] == -1
        else:
            anomaly_score = 0
            is_anomaly = False
        
        # Ensemble prediction
        ensemble_score = (cnn_prob + (1 if is_anomaly else 0)) / 2
        
        result = {
            'cnn_probability': float(cnn_prob),
            'anomaly_score': float(anomaly_score),
            'is_anomaly': bool(is_anomaly),
            'ensemble_score': float(ensemble_score),
            'prediction': 'malware' if ensemble_score > 0.5 else 'benign',
            'confidence': float(max(ensemble_score, 1 - ensemble_score))
        }
        
        return result
    
    def save_models(self, model_dir):
        """Save trained models"""
        import os
        os.makedirs(model_dir, exist_ok=True)
        
        # Save CNN model
        self.cnn_model.save(os.path.join(model_dir, 'cnn_model.h5'))
        
        # Save anomaly detector and scaler
        joblib.dump(self.anomaly_detector, os.path.join(model_dir, 'anomaly_detector.pkl'))
        joblib.dump(self.scaler, os.path.join(model_dir, 'scaler.pkl'))
        
        print(f"[+] Models saved to: {model_dir}")
    
    def load_models(self, model_dir):
        """Load trained models"""
        import os
        
        # Load CNN model
        self.cnn_model = tf.keras.models.load_model(os.path.join(model_dir, 'cnn_model.h5'))
        
        # Load anomaly detector and scaler
        self.anomaly_detector = joblib.load(os.path.join(model_dir, 'anomaly_detector.pkl'))
        self.scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
        
        self.is_trained = True
        print(f"[+] Models loaded from: {model_dir}")

# Usage example
if __name__ == "__main__":
    detector = AIMalwareDetector()
    
    # Train models (if you have a dataset)
    # train_files = ["path/to/file1.exe", "path/to/file2.exe", ...]
    # train_labels = [1, 0, 1, 0, ...]  # 1 for malware, 0 for benign
    # detector.train_models(train_files, train_labels)
    # detector.save_models("ai_models")
    
    # Load pre-trained models
    # detector.load_models("ai_models")
    
    # Predict
    # result = detector.predict("unknown_file.exe")
    # print(f"Prediction: {result['prediction']} (confidence: {result['confidence']:.2f})")
```

## ðŸ”® Kuantum DirenÃ§li Malware Analizi

### ðŸ›¡ï¸ Post-Quantum Cryptography Analysis

```python
# Quantum-Resistant Malware Analysis Framework
import hashlib
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import secrets

class QuantumResistantAnalyzer:
    def __init__(self):
        self.quantum_safe_algorithms = [
            'CRYSTALS-Kyber',
            'CRYSTALS-Dilithium',
            'FALCON',
            'SPHINCS+'
        ]
    
    def analyze_crypto_resistance(self, malware_sample):
        """Analyze cryptographic quantum resistance"""
        analysis_result = {
            'quantum_vulnerable': [],
            'quantum_resistant': [],
            'unknown_algorithms': []
        }
        
        # Detect cryptographic algorithms
        crypto_indicators = self.detect_crypto_algorithms(malware_sample)
        
        for algorithm in crypto_indicators:
            if algorithm in ['RSA', 'ECDSA', 'DH', 'ECDH']:
                analysis_result['quantum_vulnerable'].append(algorithm)
            elif algorithm in self.quantum_safe_algorithms:
                analysis_result['quantum_resistant'].append(algorithm)
            else:
                analysis_result['unknown_algorithms'].append(algorithm)
        
        return analysis_result
    
    def detect_crypto_algorithms(self, sample_data):
        """Detect cryptographic algorithm usage"""
        # Implementation would analyze binary for crypto signatures
        # This is a simplified example
        algorithms = []
        
        # Look for common crypto library signatures
        crypto_signatures = {
            b'RSA': 'RSA',
            b'ECDSA': 'ECDSA',
            b'AES': 'AES',
            b'SHA': 'SHA',
            b'Kyber': 'CRYSTALS-Kyber',
            b'Dilithium': 'CRYSTALS-Dilithium'
        }
        
        for signature, algorithm in crypto_signatures.items():
            if signature in sample_data:
                algorithms.append(algorithm)
        
        return algorithms
    
    def generate_quantum_safe_hash(self, data):
        """Generate quantum-resistant hash"""
        # Using SHA-3 which is considered quantum-resistant
        return hashlib.sha3_256(data).hexdigest()
    
    def future_proof_analysis(self, malware_sample):
        """Perform future-proof malware analysis"""
        analysis = {
            'timestamp': secrets.token_hex(16),
            'quantum_hash': self.generate_quantum_safe_hash(malware_sample),
            'crypto_analysis': self.analyze_crypto_resistance(malware_sample),
            'future_threats': self.assess_future_threats(malware_sample)
        }
        
        return analysis
    
    def assess_future_threats(self, sample_data):
        """Assess potential future threat vectors"""
        threats = {
            'quantum_decryption_risk': False,
            'ai_evasion_potential': False,
            'blockchain_abuse': False
        }
        
        # Analyze for quantum vulnerability
        crypto_analysis = self.analyze_crypto_resistance(sample_data)
        if crypto_analysis['quantum_vulnerable']:
            threats['quantum_decryption_risk'] = True
        
        # Check for AI/ML evasion techniques
        if b'adversarial' in sample_data or b'GAN' in sample_data:
            threats['ai_evasion_potential'] = True
        
        # Check for blockchain/crypto abuse
        if b'bitcoin' in sample_data or b'ethereum' in sample_data:
            threats['blockchain_abuse'] = True
        
        return threats

# Usage example
analyzer = QuantumResistantAnalyzer()
# sample_data = open('malware_sample.exe', 'rb').read()
# result = analyzer.future_proof_analysis(sample_data)
# print(f"Quantum-resistant analysis: {result}")
```

## ðŸ“š Kaynaklar ve Referanslar

### ðŸ“– Kitaplar
- "Practical Malware Analysis" - Michael Sikorski, Andrew Honig
- "The Art of Memory Forensics" - Michael Hale Ligh
- "Reversing: Secrets of Reverse Engineering" - Eldad Eilam
- "Malware Analyst's Cookbook" - Michael Ligh

### ðŸŒ Online Kaynaklar
- [SANS FOR610: Reverse-Engineering Malware](https://www.sans.org/cyber-security-courses/reverse-engineering-malware-malware-analysis-tools-techniques/)
- [Malware Analysis Tutorials](https://malwareunicorn.org/)
- [YARA Documentation](https://yara.readthedocs.io/)
- [Volatility Framework](https://www.volatilityfoundation.org/)

### ðŸ”§ AraÃ§ DokÃ¼mantasyonlarÄ±
- [IDA Pro Documentation](https://hex-rays.com/ida-pro/)
- [Ghidra User Guide](https://ghidra-sre.org/)
- [Cuckoo Sandbox Documentation](https://cuckoo.readthedocs.io/)
- [x64dbg Documentation](https://help.x64dbg.com/)

### ðŸŽ“ Sertifikasyon ProgramlarÄ±
- **GREM** (GIAC Reverse Engineering Malware)
- **GCFA** (GIAC Certified Forensic Analyst)
- **GNFA** (GIAC Network Forensic Analyst)
- **GCTI** (GIAC Cyber Threat Intelligence)

### ðŸ† CTF ve Pratik PlatformlarÄ±
- [MalwareTech Challenges](https://www.malwaretech.com/)
- [Flare-On Challenge](https://flare-on.com/)
- [PicoCTF](https://picoctf.org/)
- [OverTheWire](https://overthewire.org/)

---

## ðŸŽ¯ SonuÃ§

Bu kapsamlÄ± malware analizi eÄŸitimi, modern siber gÃ¼venlik uzmanlarÄ±nÄ±n ihtiyaÃ§ duyduÄŸu temel ve ileri dÃ¼zey becerileri kazandÄ±rmayÄ± hedeflemektedir. Statik analizden dinamik analize, tersine mÃ¼hendislikten yapay zeka destekli tespit sistemlerine kadar geniÅŸ bir yelpazede bilgi ve pratik deneyim sunmaktadÄ±r.

**Ã–nemli HatÄ±rlatmalar**:
- Malware analizi her zaman gÃ¼venli ve izole ortamlarda yapÄ±lmalÄ±dÄ±r
- Yasal ve etik kurallara uygun hareket edilmelidir
- SÃ¼rekli Ã¶ÄŸrenme ve geliÅŸim gereklidir
- Topluluk ile bilgi paylaÅŸÄ±mÄ± Ã¶nemlidir

**Gelecek AdÄ±mlar**:
1. Pratik laboratuvar ortamÄ± kurulumu
2. GerÃ§ek malware Ã¶rnekleri ile deneyim kazanma
3. Uzman topluluklar ile etkileÅŸim
4. SÃ¼rekli araÅŸtÄ±rma ve geliÅŸtirme

Malware analizi alanÄ±nda uzmanlaÅŸmak, siber gÃ¼venlik kariyerinizde Ã¶nemli bir avantaj saÄŸlayacak ve organizasyonunuzun gÃ¼venlik duruÅŸunu gÃ¼Ã§lendirecektir.